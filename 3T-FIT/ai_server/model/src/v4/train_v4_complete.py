"""
train_v4_complete.py
Complete V4 Two-Branch Model Training Pipeline

This script implements the complete V4 training pipeline:
1. Data loading and V4-specific cleaning
2. Feature engineering with intensity coefficients
3. Two-branch model training
4. Comprehensive evaluation
5. Model and artifact saving

Architecture follows README.md:
- Branch A: Dá»± Ä‘oÃ¡n CÆ°á»ng Ä‘á»™ (Intensity Prediction)
- Branch B: Dá»± Ä‘oÃ¡n NhÃ£n (Suitability Prediction)

Author: Claude Code Assistant
Date: 2025-11-27
"""

import os, re, json, argparse, numpy as np, pandas as pd, joblib
import torch, torch.nn as nn, torch.nn.functional as F
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
import warnings
warnings.filterwarnings('ignore')

# Import V4 components
from data_cleaning_v4 import V4DataCleaner
from two_branch_model_v4 import V4ModelTrainer, V4Dataset, IntensityCoefficientsCalculator
from evaluate_v4_model import V4ModelEvaluator

def main(data_dir: str, artifacts_dir: str,
         data_cleaning_path: str = None, epochs: int = 100,
         batch_size: int = 64, learning_rate: float = 1e-3,
         run_evaluation: bool = True):
    """
    Complete V4 Two-Branch Model Training Pipeline

    Args:
        data_dir: Directory containing raw data files
        artifacts_dir: Directory to save model artifacts
        data_cleaning_path: Path to pre-cleaned data (optional)
        epochs: Number of training epochs
        batch_size: Training batch size
        learning_rate: Learning rate for optimization
        run_evaluation: Whether to run comprehensive evaluation
    """
    print("="*100)
    print("V4 TWO-BRANCH MODEL COMPLETE TRAINING PIPELINE")
    print("Following README.md Architecture Specifications")
    print("="*100)
    print(f"Data Directory: {data_dir}")
    print(f"[MODEL] Artifacts Directory: {artifacts_dir}")
    print(f"[EPOCHS] Epochs: {epochs}")
    print(f"[BATCH] Batch Size: {batch_size}")
    print(f"[LR] Learning Rate: {learning_rate}")
    print("="*100)

    # Create artifacts directory
    artifacts_dir = Path(artifacts_dir)
    artifacts_dir.mkdir(parents=True, exist_ok=True)

    # ==================== STEP 1: DATA CLEANING ====================
    print("\n[1] STEP 1: V4 DATA CLEANING AND PREPROCESSING")
    print("-" * 60)

    data_cleaner = None
    cleaned_data_path = None

    if data_cleaning_path and Path(data_cleaning_path).exists():
        print(f" Using pre-cleaned data: {data_cleaning_path}")
        cleaned_data_path = data_cleaning_path
    else:
        print("[CLEAN] Running V4 data cleaning pipeline with dataset merging...")

        # Check for primary training data file
        raw_data_path = Path(data_dir) / "enhanced_gym_member_exercise_tracking_10k.xlsx"
        if not raw_data_path.exists():
            # Try alternative paths
            raw_data_path = Path(data_dir) / "enhanced_gym_member_exercise_tracking_10k_cleaned.xlsx"
            if not raw_data_path.exists():
                # Check if test_dataset exists as fallback
                raw_data_path = Path(data_dir) / "test_dataset.xlsx"

        if not raw_data_path.exists():
            raise FileNotFoundError(f"No valid data file found in {data_dir}")

        # Initialize V4 data cleaner (it will handle merging internally)
        data_cleaner = V4DataCleaner(str(raw_data_path))
        cleaned_data = data_cleaner.run_complete_cleaning_v4()
        cleaned_data_path = str(artifacts_dir / "enhanced_gym_member_exercise_tracking_v4_cleaned.xlsx")

        print(f"[OK] Data cleaning completed successfully!")
        print(f"[DIR] Cleaned data saved: {cleaned_data_path}")
        print(f" Combined dataset size: {len(cleaned_data)} samples")

    # ==================== STEP 2: MODEL TRAINING ====================
    print("\n[2] STEP 2: V4 TWO-BRANCH MODEL TRAINING")
    print("-" * 60)

    # Initialize model trainer
    trainer = V4ModelTrainer(cleaned_data_path, str(artifacts_dir / "training"))

    # Load and prepare data (with automatic dataset merging)
    print("[DIR] Loading and preparing V4 training data...")
    data = trainer.load_and_prepare_data(data_cleaner=data_cleaner, data_dir=data_dir)

    if data is None or len(data) == 0:
        raise ValueError("No valid data available for training")

    print(f"   â€¢ Loaded data shape: {data.shape}")
    print(f"   â€¢ Available columns: {list(data.columns)}")

    # Prepare datasets - Updated to 70-10-20 split as requested
    train_dataset, val_dataset, test_dataset, feature_columns, target_columns = trainer.prepare_datasets(
        train_ratio=0.7, val_ratio=0.1, test_ratio=0.2
    )

    # Determine input dimensions
    if len(train_dataset) > 0:
        sample_branch_a, sample_branch_b, _, _, _, _, _, _, _, _ = train_dataset[0]
        branch_a_input_dim = sample_branch_a.shape[0]
        branch_b_input_dim = sample_branch_b.shape[0]
    else:
        # Fallback dimensions
        branch_a_input_dim = 20  # Adjust based on your feature engineering
        branch_b_input_dim = 40  # Adjust based on your feature engineering

    print(f"   â€¢ Branch A input dimension: {branch_a_input_dim}")
    print(f"   â€¢ Branch B input dimension: {branch_b_input_dim}")
    print(f"   â€¢ Training samples: {len(train_dataset)}")
    print(f"   â€¢ Validation samples: {len(val_dataset)}")
    print(f"   â€¢ Test samples: {len(test_dataset)}")

    # Create V4 two-branch model
    model = trainer.create_model(
        branch_a_input_dim=branch_a_input_dim,
        branch_b_input_dim=branch_b_input_dim,
        shared_dim=256,
        hidden_dims=[512, 256, 128],
        dropout=0.3,
        use_attention=True
    )

    # Train model
    print(" Starting V4 Two-Branch Model Training...")
    training_history = trainer.train_model(
        train_dataset, val_dataset,
        epochs=epochs,
        batch_size=batch_size,
        learning_rate=learning_rate
    )

    # Save model and artifacts
    trainer.save_model_and_artifacts(feature_columns, target_columns)

    model_path = artifacts_dir / "training" / "v4_two_branch_model.pt"
    print(f"[OK] V4 Model training completed successfully!")
    print(f"[DIR] Model saved: {model_path}")

    # ==================== STEP 3: COMPREHENSIVE EVALUATION ====================
    if run_evaluation:
        print("\n[3] STEP 3: COMPREHENSIVE V4 MODEL EVALUATION")
        print("-" * 60)

        # Initialize evaluator
        evaluator = V4ModelEvaluator(str(model_path), str(artifacts_dir / "evaluation"))

        # Load test data
        print("[DIR] Preparing test data for evaluation...")

        # Create test dataset with same feature specs
        intensity_calculator = IntensityCoefficientsCalculator()
        test_data_for_eval = trainer.data.iloc[len(train_dataset) + len(val_dataset):]

        # Ensure test data has required columns
        if len(test_data_for_eval) == 0:
            print("[WARNING] No test data available, using validation data for evaluation")
            test_data_for_eval = trainer.data.iloc[len(train_dataset):len(train_dataset) + len(val_dataset)]

        # Convert test data to dataset
        test_dataset_eval = V4Dataset(test_data_for_eval, feature_columns, target_columns, intensity_calculator)

        if len(test_dataset_eval) == 0:
            print("[ERROR] No evaluation data available")
            return

        print(f"   â€¢ Evaluation samples: {len(test_dataset_eval)}")

        # Run complete evaluation
        evaluation_results = evaluator.run_complete_evaluation(test_dataset_eval)

        # ==================== STEP 4: FINAL SUMMARY ====================
        print("\n[4] STEP 4: TRAINING AND EVALUATION SUMMARY")
        print("=" * 100)

        # Model Configuration
        print("[MODEL] Model Configuration:")
        print(f"   â€¢ Model Type: V4 Two-Branch")
        print(f"   â€¢ Branch A Input: {branch_a_input_dim} features")
        print(f"   â€¢ Branch B Input: {branch_b_input_dim} features")
        print(f"   â€¢ Shared Dimension: 256")
        print(f"   â€¢ Hidden Layers: [512, 256, 128]")
        print(f"   â€¢ Dropout: 0.3")
        print(f"   â€¢ Attention: Enabled")
        print(f"   â€¢ Multi-task: Intensity, Suitability, Readiness, Performance")

        # Training Results
        print("\n Training Results:")
        print(f"   â€¢ Total Epochs: {training_history['total_epochs']}")
        print(f"   â€¢ Best Validation Loss: {training_history['best_val_loss']:.6f}")
        print(f"   â€¢ Final Training Loss: {training_history['train_losses'][-1]:.6f}")
        print(f"   â€¢ Final Validation Loss: {training_history['val_losses'][-1]:.6f}")

        # Evaluation Results
        if 'performance_metrics' in evaluation_results:
            metrics = evaluation_results['performance_metrics']

            print("\n Evaluation Results:")

            # Branch A: Intensity Prediction
            print("\n   Branch A: Intensity Prediction")
            print(f"   â€¢ RMSE: {metrics['intensity']['rmse']:.4f}")
            print(f"   â€¢ RÂ²: {metrics['intensity']['r2']:.4f}")
            print(f"   â€¢ MAE: {metrics['intensity']['mae']:.4f}")
            print(f"   â€¢ Pearson Correlation: {metrics['intensity']['pearson_r']:.4f}")

            # Branch B: Suitability Prediction
            print("\n   Branch B: Suitability Prediction")
            print(f"   â€¢ RMSE: {metrics['suitability']['rmse']:.4f}")
            print(f"   â€¢ RÂ²: {metrics['suitability']['r2']:.4f}")
            print(f"   â€¢ MAE: {metrics['suitability']['mae']:.4f}")
            print(f"   â€¢ Classification Accuracy (â‰¥0.75): {metrics['suitability']['accuracy']:.1%}")
            print(f"   â€¢ AUC: {metrics['suitability']['auc']:.4f}")

            # Multi-task Performance
            print("\n   Multi-task Performance:")
            print(f"   â€¢ Readiness RMSE: {metrics['readiness']['rmse']:.4f}")
            print(f"   â€¢ Readiness RÂ²: {metrics['readiness']['r2']:.4f}")
            print(f"   â€¢ Performance RMSE: {metrics['performance']['rmse']:.4f}")
            print(f"   â€¢ Performance RÂ²: {metrics['performance']['r2']:.4f}")
            print(f"   â€¢ Overall Composite Score: {metrics['overall']['composite_score']:.4f}")

            # Suitability Score Distribution
            print("\n   Suitability Score Distribution (V4 README.md):")
            score_dist = metrics['suitability']['score_distribution']
            for category, percentage in score_dist.items():
                emoji = {
                    "Perfect Fit": "ðŸŸ£",
                    "Very Good": "",
                    "Good": "ðŸŸ¢",
                    "Moderate": "ðŸŸ¡",
                    "Low": "[WARNING]",
                    "Ineffective": "[ERROR]"
                }.get(category, "")
                print(f"   â€¢ {emoji} {category}: {percentage:.1f}%")

        # Baseline Comparison
        if 'baseline_comparison' in evaluation_results:
            comparison = evaluation_results['baseline_comparison']
            print("\n Performance vs Baseline:")
            print(f"   â€¢ Overall Improvement: +{comparison['overall_comparison']['improvement_composite']:.1f}%")
            print(f"   â€¢ Intensity RÂ² Improvement: +{comparison['intensity_comparison']['improvement_r2']:.1f}%")
            print(f"   â€¢ Suitability RÂ² Improvement: +{comparison['suitability_comparison']['improvement_r2']:.1f}%")
            print(f"   â€¢ Readiness RÂ² Improvement: +{comparison['readiness_comparison']['improvement_r2']:.1f}%")

        # Workout Generation
        if 'workout_results' in evaluation_results:
            workout_stats = evaluation_results['workout_results']
            total_workouts = len(workout_stats)
            successful_workouts = sum(1 for w in workout_stats if w['workout_quality']['is_appropriate'])
            avg_quality = np.mean([w['workout_quality']['overall_score'] for w in workout_stats])

            print("\n[MODEL] Workout Generation Results:")
            print(f"   â€¢ Total Workouts Generated: {total_workouts}")
            print(f"   â€¢ Success Rate: {successful_workouts/total_workouts:.1%}")
            print(f"   â€¢ Average Quality Score: {avg_quality:.3f}")

        # Artifacts Location
        print(f"\n All Artifacts Saved To:")
        print(f"   â€¢ Main Directory: {artifacts_dir}")
        print(f"   â€¢ Model: {model_path}")
        print(f"   â€¢ Data Cleaning: {cleaned_data_path}")
        print(f"   â€¢ Training Artifacts: {artifacts_dir / 'training'}")
        print(f"   â€¢ Evaluation Artifacts: {artifacts_dir / 'evaluation'}")
        print(f"   â€¢ Visualizations: {artifacts_dir / 'evaluation' / 'visualizations'}")

        # Recommendations
        print(f"\n Key Findings and Recommendations:")

        if 'performance_metrics' in evaluation_results:
            metrics = evaluation_results['performance_metrics']

            # Intensity Prediction Assessment
            if metrics['intensity']['r2'] >= 0.8:
                print("   [OK] Intensity prediction shows excellent performance (RÂ² â‰¥ 0.8)")
            elif metrics['intensity']['r2'] >= 0.7:
                print("   [OK] Intensity prediction shows good performance (RÂ² â‰¥ 0.7)")
            elif metrics['intensity']['r2'] >= 0.6:
                print("   [WARNING] Intensity prediction shows moderate performance (RÂ² â‰¥ 0.6) - consider more training or feature engineering")
            else:
                print("   [ERROR] Intensity prediction needs improvement (RÂ² < 0.6) - consider architecture changes or more data")

            # Suitability Prediction Assessment
            if metrics['suitability']['r2'] >= 0.8:
                print("   [OK] Suitability prediction shows excellent performance (RÂ² â‰¥ 0.8)")
            elif metrics['suitability']['r2'] >= 0.7:
                print("   [OK] Suitability prediction shows good performance (RÂ² â‰¥ 0.7)")
            elif metrics['suitability']['r2'] >= 0.6:
                print("   [WARNING] Suitability prediction shows moderate performance (RÂ² â‰¥ 0.6) - consider more training")
            else:
                print("   [ERROR] Suitability prediction needs improvement (RÂ² < 0.6)")

            # Overall Assessment
            overall_score = metrics['overall']['composite_score']
            if overall_score >= 0.8:
                print(f"    Overall model performance is EXCELLENT (Composite: {overall_score:.3f})")
            elif overall_score >= 0.7:
                print(f"    Overall model performance is GOOD (Composite: {overall_score:.3f})")
            elif overall_score >= 0.6:
                print(f"    Overall model performance is ACCEPTABLE (Composite: {overall_score:.3f})")
            else:
                print(f"   [WARNING] Overall model performance needs improvement (Composite: {overall_score:.3f})")

        # Success Indicators
        print(f"\n V4 Two-Branch Model Training Pipeline Completed Successfully!")
        print(f"[OK] All components trained and evaluated according to README.md specifications")
        print(f" Comprehensive evaluation report and visualizations generated")
        print(f"[MODEL] Rule-based workout generation implemented and validated")
        print(f"[EPOCHS] Multi-task learning coordination verified")

        return {
            'model_path': str(model_path),
            'cleaned_data_path': cleaned_data_path,
            'training_history': training_history,
            'evaluation_results': evaluation_results,
            'artifacts_directory': str(artifacts_dir)
        }

    else:
        print("\n[WARNING] Evaluation skipped as requested")
        return {
            'model_path': str(model_path),
            'cleaned_data_path': cleaned_data_path,
            'training_history': training_history,
            'artifacts_directory': str(artifacts_dir)
        }


def create_sample_data_if_needed(data_dir: str):
    """Create sample data if no data files exist (for testing)"""
    import random

    print("[WARNING] No data files found, creating sample data for testing...")

    data_dir_path = Path(data_dir)
    data_dir_path.mkdir(parents=True, exist_ok=True)

    # Generate synthetic data
    np.random.seed(42)
    random.seed(42)

    n_samples = 1000

    # Basic demographics
    data = {
        'age': np.random.randint(18, 65, n_samples),
        'weight_kg': np.random.normal(75, 15, n_samples),
        'height_m': np.random.normal(1.75, 0.1, n_samples),
        'bmi': np.random.normal(24.5, 3.5, n_samples),
        'gender': np.random.choice(['male', 'female'], n_samples),
        'experience_level': np.random.randint(1, 4, n_samples),
        'workout_frequency': np.random.randint(1, 7, n_samples),
        'resting_heartrate': np.random.normal(65, 8, n_samples),
        'estimated_1rm': np.random.normal(80, 30, n_samples)
    }

    # Ensure BMI is calculated properly
    data['bmi'] = data['weight_kg'] / (data['height_m'] ** 2)

    # SePA features
    sepa_moods = ['Very Bad', 'Bad', 'Neutral', 'Good', 'Very Good', 'Excellent']
    sepa_levels = ['Very Low', 'Low', 'Medium', 'High', 'Very High']

    data['mood'] = np.random.choice(sepa_moods, n_samples)
    data['fatigue'] = np.random.choice(sepa_levels, n_samples)
    data['effort'] = np.random.choice(sepa_levels, n_samples)

    # Exercise data
    exercises = ['Bench Press', 'Squat', 'Deadlift', 'Running', 'Cycling', 'Push-ups', 'Pull-ups']
    workout_data = []

    for _ in range(n_samples):
        num_exercises = random.randint(1, 3)
        selected_exercises = random.sample(exercises, num_exercises)

        exercise_strs = []
        for exercise in selected_exercises:
            if exercise in ['Running', 'Cycling']:
                # Cardio exercises
                distance = random.uniform(1, 10)  # km
                time = random.uniform(5, 30)    # minutes
                exercise_str = f"{int(time*60)}x{distance:.1f}x1"  # reps x weight x sets
            else:
                # Strength exercises
                reps = random.randint(5, 20)
                weight = random.uniform(20, 120)
                sets = random.randint(1, 5)
                exercise_str = f"{reps}x{weight:.1f}x{sets}"

            exercise_strs.append(exercise_str)

        workout_data.append(' | '.join(exercise_strs))

    data['workout_data'] = workout_data
    data['exercise_name'] = np.random.choice(exercises, n_samples)

    # Goals
    data['fitness_goal'] = np.random.choice(['strength', 'hypertrophy', 'endurance', 'general_fitness'], n_samples)

    # Targets (will be computed by data cleaner)
    data['suitability_score'] = np.random.beta(2, 2, n_samples)  # Beta distribution for 0-1 range

    # Health indicators
    data['heart_rate_avg'] = np.random.normal(85, 12, n_samples)
    data['heart_rate_max'] = np.random.normal(160, 20, n_samples)
    data['steps'] = np.random.normal(8000, 3000, n_samples)
    data['distance'] = np.random.normal(5, 3, n_samples)
    data['calories_burned'] = np.random.normal(300, 100, n_samples)
    data['vo2max'] = np.random.normal(35, 10, n_samples)
    data['sleep_duration'] = np.random.normal(7, 1.5, n_samples)

    # Create DataFrame
    df = pd.DataFrame(data)

    # Ensure positive values
    df['weight_kg'] = np.maximum(df['weight_kg'], 35)
    df['height_m'] = np.maximum(df['height_m'], 1.2)
    df['estimated_1rm'] = np.maximum(df['estimated_1rm'], 15)
    df['resting_heartrate'] = np.maximum(df['resting_heartrate'], 40)
    df['steps'] = np.maximum(df['steps'], 0)
    df['distance'] = np.maximum(df['distance'], 0)
    df['calories_burned'] = np.maximum(df['calories_burned'], 0)
    df['vo2max'] = np.maximum(df['vo2max'], 10)
    df['sleep_duration'] = np.maximum(df['sleep_duration'], 4)

    # Save data
    data_path = data_dir_path / "enhanced_gym_member_exercise_tracking_10k.xlsx"
    df.to_excel(data_path, index=False)

    print(f"[OK] Sample data created: {data_path}")
    print(f"   â€¢ Samples: {len(df)}")
    print(f"   â€¢ Features: {len(df.columns)}")

    return str(data_path)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='V4 Two-Branch Model Complete Training Pipeline')

    parser.add_argument('--data_dir', type=str, default='./data',
                       help='Directory containing data files')
    parser.add_argument('--artifacts_dir', type=str, default='./artifacts_v4',
                       help='Directory to save model artifacts')
    parser.add_argument('--cleaned_data', type=str, default=None,
                       help='Path to pre-cleaned data file (optional)')
    parser.add_argument('--epochs', type=int, default=100,
                       help='Number of training epochs')
    parser.add_argument('--batch_size', type=int, default=64,
                       help='Training batch size')
    parser.add_argument('--learning_rate', type=float, default=1e-3,
                       help='Learning rate')
    parser.add_argument('--skip_evaluation', action='store_true',
                       help='Skip comprehensive evaluation')
    parser.add_argument('--create_sample_data', action='store_true',
                       help='Create sample data if none exists')

    args = parser.parse_args()

    try:
        # Check if data exists, create sample if requested
        data_dir_path = Path(args.data_dir)
        if not any(data_dir_path.glob("*.xlsx")):
            if args.create_sample_data:
                args.cleaned_data = create_sample_data_if_needed(args.data_dir)
            else:
                print(f"[ERROR] No data files found in {args.data_dir}")
                print(" Use --create_sample_data to generate sample data for testing")
                exit(1)

        # Run complete training pipeline
        results = main(
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
            data_cleaning_path=args.cleaned_data,
            epochs=args.epochs,
            batch_size=args.batch_size,
            learning_rate=args.learning_rate,
            run_evaluation=not args.skip_evaluation
        )

        if results:
            print(f"\n V4 Training Pipeline Completed Successfully!")
            print(f"[DIR] Check results in: {results['artifacts_directory']}")
        else:
            print(f"\n[ERROR] V4 Training Pipeline Failed")

    except KeyboardInterrupt:
        print(f"\n[WARNING] Training pipeline interrupted by user")
    except Exception as e:
        print(f"\n[ERROR] Error during V4 training pipeline: {e}")
        import traceback
        traceback.print_exc()